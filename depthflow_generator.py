import os
import sys
import argparse
import json
import gc
import numpy as np
import torch
import torch.nn.functional as F
from pathlib import Path
from PIL import Image, ImageFilter

# ==========================================
# ğŸš‘ å…¼å®¹æ€§è¡¥ä¸ï¼šä¿®å¤ PyTorch 2.1.2 å…¼å®¹æ€§
# ==========================================
try:
    import torch.utils._pytree as _pytree

    if not hasattr(_pytree, "register_pytree_node") and hasattr(_pytree, "_register_pytree_node"):
        _pytree.register_pytree_node = _pytree._register_pytree_node
except:
    pass

# å¼•å…¥æ¨¡å‹åº“
from transformers import AutoModelForDepthEstimation, AutoImageProcessor, AutoModelForImageSegmentation
from diffusers import StableDiffusionInpaintPipeline

# === è·¯å¾„é…ç½® ===
BASE_DIR = Path(__file__).parent.absolute()
MODEL_DIR = BASE_DIR / "models"
OUTPUT_DIR = BASE_DIR / "output"

# æ£€æŸ¥æœ¬åœ°æ¨¡å‹
PATH_DEPTH = MODEL_DIR / "depth_anything_v2"
PATH_SEG = MODEL_DIR / "rmbg_1_4"
PATH_SD = MODEL_DIR / "sd_inpainting"

if not PATH_DEPTH.exists() or not PATH_SEG.exists() or not PATH_SD.exists():
    print(f"âŒ Error: Local models not found in {MODEL_DIR}")
    sys.exit(1)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âš™ï¸ Running on device: {DEVICE} (Torch: {torch.__version__})")


# === è¾…åŠ©å‡½æ•° ===
def cleanup():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


# === æ¨¡å‹åŠ è½½ (çº¯æœ¬åœ°) ===

def get_depth_utils():
    print(f"Loading Depth Model from: {PATH_DEPTH.name}")
    processor = AutoImageProcessor.from_pretrained(PATH_DEPTH, local_files_only=True)
    model = AutoModelForDepthEstimation.from_pretrained(PATH_DEPTH, local_files_only=True).to(DEVICE)
    return model, processor


def get_seg_model():
    print(f"Loading Seg Model from: {PATH_SEG.name}")
    model = AutoModelForImageSegmentation.from_pretrained(PATH_SEG, trust_remote_code=True, local_files_only=True).to(
        DEVICE)
    model.eval()
    return model


def get_inpainting_pipe():
    print(f"Loading SD Pipeline from: {PATH_SD.name}")
    pipe = StableDiffusionInpaintPipeline.from_pretrained(
        PATH_SD,
        torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
        local_files_only=True,
        use_safetensors=True,
        variant="fp16"
    ).to(DEVICE)
    if DEVICE == "cuda":
        pipe.enable_attention_slicing()
    return pipe


# === æ ¸å¿ƒé€»è¾‘ ===

def estimate_depth(image_pil):
    """ç”Ÿæˆæ·±åº¦å›¾"""
    model, processor = get_depth_utils()

    if image_pil.mode != "RGB":
        image_pil = image_pil.convert("RGB")

    inputs = processor(images=image_pil, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        depth = model(**inputs).predicted_depth

    h, w = image_pil.size[::-1]
    depth = torch.nn.functional.interpolate(
        depth.unsqueeze(1), size=(h, w), mode="bicubic", align_corners=False
    )

    depth_min, depth_max = depth.min(), depth.max()
    depth_norm = (depth - depth_min) / (depth_max - depth_min)
    depth_uint8 = (depth_norm * 255.0).cpu().numpy().astype(np.uint8)[0, 0]

    del model, processor, inputs, depth
    cleanup()

    return Image.fromarray(depth_uint8, mode="L")


def generate_mask(image_pil):
    """RMBG-1.4 åˆ†å‰²"""
    model = get_seg_model()

    orig_w, orig_h = image_pil.size
    input_size = (1024, 1024)

    im_tensor = image_pil.resize(input_size, Image.BILINEAR)
    im_arr = np.array(im_tensor).astype(np.float32) / 255.0
    im_arr = (im_arr - 0.5) / 0.5
    im_tensor = torch.from_numpy(im_arr).permute(2, 0, 1).unsqueeze(0).float().to(DEVICE)

    with torch.no_grad():
        preds = model(im_tensor)

    while isinstance(preds, (list, tuple)):
        preds = preds[0]

    if hasattr(preds, 'pred'):
        preds = preds.pred
    elif hasattr(preds, 'logits'):
        preds = preds.logits

    preds = F.interpolate(preds, size=(orig_h, orig_w), mode='bilinear', align_corners=False)

    result = (preds[0][0] > 0).cpu().numpy()
    if preds.max() <= 1.0:
        result = (preds[0][0] > 0.5).cpu().numpy()

    mask_pil = Image.fromarray((result * 255).astype('uint8')).convert("L")

    del model, im_tensor, preds
    cleanup()
    return mask_pil


def get_smart_inpaint_mask(mask_pil, image_size, max_parallax_percent=0.04):
    """
    è®¡ç®—æ™ºèƒ½ä¿®è¡¥é®ç½© (Rim Mask) - æ€§èƒ½ä¼˜åŒ–ç‰ˆ
    å…ˆç¼©å°å¤„ç†å†æ”¾å¤§ï¼Œè§£å†³å¤§åˆ†è¾¨ç‡ä¸‹ PIL MaxFilter å¡æ­»çš„é—®é¢˜
    """
    w, h = image_size

    # === å…³é”®ä¼˜åŒ– ===
    # å°†å¤„ç†åˆ†è¾¨ç‡é™åˆ¶åœ¨ 1024 åƒç´ ä»¥å†…
    # PIL çš„ MaxFilter ç®—æ³•å¤æ‚åº¦éšåŠå¾„å¹³æ–¹å¢é•¿ï¼Œç¼©å°å¤„ç†å¯æé€Ÿç™¾å€
    process_max_dim = 1024
    scale_factor = 1.0

    if max(w, h) > process_max_dim:
        scale_factor = process_max_dim / max(w, h)
        process_w = int(w * scale_factor)
        process_h = int(h * scale_factor)
        # ä½¿ç”¨ Nearest ç¼©æ”¾é®ç½©ä»¥ä¿æŒäºŒå€¼ç‰¹æ€§
        mask_processing = mask_pil.resize((process_w, process_h), Image.Resampling.NEAREST)
    else:
        process_w, process_h = w, h
        mask_processing = mask_pil

    # åœ¨ç¼©å°åçš„å°ºå¯¸ä¸Šè®¡ç®—åç§»é‡
    offset_px = int(min(process_w, process_h) * max_parallax_percent)

    # 1. å¤–æ‰© (Dilation)
    mask_dilated = mask_processing.filter(ImageFilter.MaxFilter(size=offset_px * 2 + 1))

    # 2. å†…ç¼© (Erosion)
    safe_zone_radius = int(offset_px * 1.5)
    mask_eroded = mask_processing.filter(ImageFilter.MinFilter(size=safe_zone_radius * 2 + 1))

    # 3. è®¡ç®—ç¯å½¢åŒºåŸŸ
    arr_dilated = np.array(mask_dilated).astype(np.float32)
    arr_eroded = np.array(mask_eroded).astype(np.float32)

    arr_final = arr_dilated - arr_eroded
    arr_final = np.clip(arr_final, 0, 255)

    # ç‰¹æ®Šæƒ…å†µå¤„ç†
    if np.sum(arr_eroded) < 100:
        arr_final = arr_dilated

    result = Image.fromarray(arr_final.astype(np.uint8), mode="L")

    # === æ¢å¤åŸå§‹å°ºå¯¸ ===
    if scale_factor != 1.0:
        # æ”¾å¤§å›å»ï¼Œä½¿ç”¨ Bilinear è®©è¾¹ç¼˜ç¨å¾®å¹³æ»‘ä¸€ç‚¹ç‚¹
        result = result.resize((w, h), Image.Resampling.BILINEAR)

    return result

def generate_background(image_pil, mask_pil, prompt):
    """SD Inpainting (æ™ºèƒ½è¾¹ç¼˜ä¿®è¡¥ç‰ˆ)"""

    # å¼ºåˆ¶ RGB
    if image_pil.mode != "RGB":
        image_pil = image_pil.convert("RGB")

    w, h = image_pil.size

    print("ğŸ§  Calculating parallax-aware inpaint mask...")
    # è®¡ç®—æ™ºèƒ½é®ç½©
    smart_mask = get_smart_inpaint_mask(mask_pil, (w, h), max_parallax_percent=0.04)

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®è¡¥
    mask_arr = np.array(smart_mask)
    inpaint_area_ratio = np.sum(mask_arr > 128) / mask_arr.size
    print(f"â„¹ï¸ Inpaint Area Ratio: {inpaint_area_ratio:.1%}")

    if inpaint_area_ratio < 0.001:
        print("âš¡ Subject is static or too small, skipping inpainting.")
        return image_pil

    pipe = get_inpainting_pipe()

    # ç¼©æ”¾è‡³ 8 çš„å€æ•° (æœ€é«˜ 1024)
    process_w = 1024 if w > 1024 else (w // 8) * 8
    process_h = 1024 if h > 1024 else (h // 8) * 8

    img_in = image_pil.resize((process_w, process_h), Image.Resampling.LANCZOS)
    mask_in = smart_mask.resize((process_w, process_h), Image.Resampling.NEAREST)

    print("ğŸ¨ Generating background (Rim Inpainting)...")
    result = pipe(
        prompt=prompt,
        negative_prompt="bad quality, distorted, ugly, text, watermark, foreground object, person, clothes, skin",
        image=img_in,
        mask_image=mask_in,
        num_inference_steps=25,
        guidance_scale=7.5,
        strength=1.0  # 100% é‡ç»˜é®ç½©åŒºåŸŸ
    ).images[0]

    del pipe
    cleanup()

    # æ¢å¤åŸå§‹å°ºå¯¸
    result = result.resize((w, h), Image.Resampling.LANCZOS)

    # === å…³é”®æ­¥éª¤ï¼šåˆæˆ ===
    # ä»…æ›¿æ¢ smart_mask è¦†ç›–çš„åŒºåŸŸ (è¾¹ç¼˜)ï¼Œä¿ç•™åŸå§‹èƒŒæ™¯å’Œç‰©ä½“æ·±å±‚ä¸­å¿ƒ
    # è¿™æ ·å¯ä»¥é˜²æ­¢èƒŒæ™¯é—ªçƒï¼Œå¹¶è§£å†³å¤§ç‰©ä½“ä¿®è¡¥å›°éš¾çš„é—®é¢˜
    final_comp = Image.composite(result, image_pil, smart_mask)

    return final_comp


# === ä¸»æµç¨‹ ===

def main(input_path, output_dir, prompt):
    input_file = Path(input_path)
    if not input_file.exists():
        print(f"âŒ Input file not found: {input_file}")
        return

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    print(f"ğŸš€ Processing: {input_file.name}")

    img = Image.open(input_file).convert("RGB")

    print("\n--- Step 1: Foreground Depth ---")
    depth_fg = estimate_depth(img)

    print("\n--- Step 2: Segmentation (Mask) ---")
    mask = generate_mask(img)

    print("\n--- Step 3: Background Generation ---")
    img_bg = generate_background(img, mask, prompt)

    print("\n--- Step 4: Background Depth ---")
    depth_bg = estimate_depth(img_bg)

    print("\nğŸ’¾ Saving assets...")
    assets = {
        "image": img,
        "depth": depth_fg,
        "image_bg": img_bg,
        "depth_bg": depth_bg,
        "subject_mask": mask
    }

    for name, pil_obj in assets.items():
        pil_obj.save(output_path / f"{name}.png")

    config = {
        "height": 0.20,
        "steady": 0.0,
        "focus": 0.0,
        "zoom": 1.0,
        "isometric": 0.0,
        "offset_x": 0.0,
        "offset_y": 0.0,
        "resolution": img.size
    }
    with open(output_path / "config.json", "w") as f:
        json.dump(config, f, indent=4)

    print(f"âœ… Success! Assets saved to: {output_path.absolute()}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-i", "--input", required=True, help="Input image path")
    parser.add_argument("-o", "--output", default="output", help="Output directory")
    parser.add_argument("-p", "--prompt", default="background, nature, realistic, high quality")
    args = parser.parse_args()

    main(args.input, args.output, args.prompt)